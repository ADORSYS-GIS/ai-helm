global:
  version: "latest"
  labels:
    app: lmcache-kserve-inference
    team: adorsys-gis

inferenceService:
  name: "lmcache-inference-service"
  replicaCount: 1
  model:
    image: "lmcache/vllm-openai:latest"
    imagePullPolicy: IfNotPresent
    storageUri: "gs://your-bucket/model"
  resources:
    requests:
      memory: "4Gi"
      cpu: "2000m"
      nvidia.com/gpu: "1"
    limits:
      memory: "8Gi"
      cpu: "4000m"
      nvidia.com/gpu: "1"
  
lmcache:
  chunkSize: "256"
  localCpu: "True"
  localDisk: ""  # Set to "file:///path/to/cache" to enable
  maxLocalDiskSize: "0.0"  # GB, set > 0 to enable disk caching
  remoteUrl: ""  # Set to "protocol://host:port" to enable
  enableP2P: "False"
  lookupUrl: ""  # Required if enableP2P is True
  distributedUrl: ""  # Required if enableP2P is True

httpRoute:
  enabled: false

ingress:
  enabled: false
  annotations: {}
  hosts:
    - host: lmcache.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: lmcache-example-com-secret
      hosts:
        - lmcache.example.com

service:
  type: ClusterIP
  port: 80
  annotations: {}
